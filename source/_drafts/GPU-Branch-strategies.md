---
title: GPU Flow-Control Idioms 笔记
tags:
- graphics
- GPU
---

一直只知道shader中用分支会有额外开销，但总想搞清楚，GPU执行分支的时候发生了什么。这点不同架构，不同厂商的GPU底层细节可能都不同，他们也不会公开太多消息。我找到[GPU Gems2](https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-34-gpu-flow-control-idioms)的一篇老文章学习（貌似是好多年前的，里面还在讲GeForce 6系列显卡，不过基本原理应该还是有参考价值的），做笔记记录一下。

## CPU和GPU的分支模型

首先看看CPU分支的性能开销，CPU的指令流水线通常比较长（能达到10到20个时钟周期），为了让效率最大化，理想情况是不希望流水线被打断的。所以CPU的[分支预测](https://en.wikipedia.org/wiki/Branch_predictor)很重要。如果没有分支预测，在条件跳转（condition jump）执行完毕之前，CPU必须等待，流水线只能空着。为了改善这个问题，CPU会对分支进行预测，根据结果提前让指令进入流水线执行，如果预测失败，也不过是把提前执行的指令中断，重新让正确的指令进入流水线。

在GPU上，最新的GPU（那时候还是GeForce 6系列）也有类似的分支指令，但性能与CPU有些许不同。更老的GPU则没有原生的分支指令，而是用其他方式模拟分支的行为。并行架构中最常见的机制是SIMD（单指令多数据）和MIMD（多指令多数据）。SIMD架构中的所有处理单元在同一时刻执行相同的指令。MIMD中不同的处理器则可以同时执行不同的指令。

GPU目前用三种方式来实现分支：MIMD分支，SIMD分支和条件码。

*MIMD分支*

MIMD是最理想的情况，每个处理单元可以像GPU那样，在没有性能损失的情况下执行不同的分支代码，GeForce 6系列的顶点处理器中支持MIMD分支。

*SIMD分支*

SIMD分支支持程序中出现分支以及循环，但由于所有处理器执行完全一致的指令，所以不同处理器执行出现分歧(divergent)时性能会损失。举个例子，如果一个片元着色器在条件判断中，根据输入的随机数确定执行哪个分支，然后输出不同的值。那么条件不为真的处理器，在条件为真的处理器执行完毕之前就必须等待（SIMD中的一组处理器，会先全部执行true的指令，对于不为true的processor，会抛弃这些结果。然后再全部执行false的指令）。结果就是总时间等于两个分支的总和加上条件判断指令的时间。

![SIMD的执行模型]()

因此SIMD在分支条件的分布比较“连贯”的时候相当有用（比如大多数warp里的处理器都走相同的分支，只有少数发生分歧），如果条件分布很不“连贯”（比如每个warp里面有一个处理器走不同的分值），性能会严重下降。

*条件码*

在更老的GPU中采用条件码的方式来模拟分支的行为，条件判断执行后会设置条件码，if的true和false分支全都会被执行（不像SIMD中只有发生分歧才会这样），随后根据条件码的状态决定采用哪个分支的结果。所以这种架构中，分支的代价必然等于所有分支的执行时间之和加上条件判断的时间，如果每个分支的指令很少，条件码也能比较高效的执行，当分支指令变得很复杂时，就需要采取其他方式了。所以这种架构中应该尽可能的避免分支。

## 控制流的基本策略

### 将分支前移

由于GPU的分支的机制相当复杂，我们需要有不同策略来应对各种情况，一种比较有效的就是将条件判断尽可能前移，在越早的阶段做判断效率就越高。

*静态分支的粒度*

在CPU上对数组或流式数据进行计算